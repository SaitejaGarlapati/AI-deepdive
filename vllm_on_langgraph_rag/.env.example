HF_TOKEN=

# where to cache models on the host; mounted into the container
HF_HOME=~/hf_cache
HPU_RECIPE_CACHE=~/hpu_recipe_cache

# If using local vLLM (Habana vLLM fork OpenAI server):
VLLM_API_BASE=http://localhost:8000/v1
VLLM_API_KEY=dummy
VLLM_MODEL=meta-llama/Meta-Llama-3-8B-Instruct

# Switch between local vLLM (OpenAI-compatible) and real OpenAI
# Use one of: "vllm" or "openai"
LLM_PROVIDER=vllm


# If using real OpenAI (optional fallback):
# OPENAI_API_KEY=sk-...
# OPENAI_MODEL=gpt-4o-mini